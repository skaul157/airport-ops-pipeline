# Real‑Time Airport Operations Data Pipeline

![Pipeline Diagram](visualizations/updated_architecture_diagram.png)

## Overview

This project demonstrates how to build a **production‑ready data pipeline** on Azure using streaming ingestion, robust ETL, Delta Lake storage and interactive dashboards.  The solution draws on my experience modernizing airport analytics platforms: processing **30 TB+ of operational data per month**, cutting report refresh times from hours to minutes and boosting SLA compliance for baggage and gate teams.  By following the steps in this repository you can stand up a similar pipeline on your own Azure subscription or run it locally with synthetic data.

The goal is to showcase end‑to‑end data engineering capabilities – ingestion, quality checks, transformation, storage optimisation, infrastructure as code and visualization – all within a single, easy‑to‑navigate repo.  The materials here are designed for hiring managers, venture capital partners and technical peers evaluating my expertise in modern data platforms.

## Key Use Cases

This pipeline addresses several real‑world scenarios encountered at busy airports:

* **Real‑Time Flight Monitoring** – ingest and analyse live departure/arrival events to compute on‑time performance and send alerts when flights are delayed beyond a threshold.
* **Baggage Handling Analytics** – track baggage counts and throughput by flight and terminal to optimise staffing and equipment utilisation.
* **Incident Management & SLA Tracking** – ingest ServiceNow incident tickets (baggage delays, technical issues, passenger disruptions) and correlate them with flight and gate data to monitor SLA compliance.
* **Historical Reporting** – store cleansed and aggregated data in Delta Lake for long‑term analysis and regulatory reporting.

## Tech Stack

| Category | Technologies |
|---|---|
| **Ingestion & Orchestration** | Azure Event Hubs, Azure Data Factory, Apache Kafka (optional) |
| **Processing & Storage** | Azure Databricks (PySpark), Delta Lake, Azure Data Lake Storage Gen2 |
| **Infrastructure as Code** | Terraform (see `infra/main.tf`) or Bicep |
| **Data Quality & Logging** | Great Expectations, Azure Monitor (see `scripts/logging_sample.py`) |
| **Visualization & Analytics** | Power BI, Azure Synapse Analytics |

## Architecture

The updated diagram above illustrates the end‑to‑end flow using Azure‑native components:

1. **Sources** – flight events stream in via **Event Hubs** and operational incidents are captured from **ServiceNow** APIs.
2. **Azure Data Factory** orchestrates ingestion by reading from Event Hubs and landing raw events in the **Bronze** layer of a Data Lake.
3. **Azure Databricks** notebooks (PySpark) parse, clean and join data with reference tables.  The cleansed data forms the **Silver** layer and is validated using **Great Expectations**.
4. Aggregations such as KPIs and time‑series metrics are computed in the **Gold** layer and written as Delta tables.  These tables feed **Azure Synapse** for ad‑hoc queries and **Power BI** dashboards for business users.
5. **Monitoring & Logging** using Azure Monitor captures pipeline performance metrics and log events for troubleshooting and alerting.

## Pipeline Stages

The code in this repository is organised around the classic Bronze–Silver–Gold lakehouse pattern:

1. **Ingestion** – `scripts/ingestion.py` reads CSV/JSON files (or streams from Event Hubs) and writes raw Delta tables to `data/staging/bronze`.
2. **Staging & Cleaning** – `scripts/transformation.py` parses timestamps, normalises schemas, handles missing values and joins against reference data such as airport codes.  The resulting Silver tables live in `data/staging/silver`.
3. **Transformation & Aggregation** – the same script aggregates Silver data into Gold tables (e.g. average departure delays, incident rates) stored in `data/gold`.
4. **Data Quality** – `scripts/data_quality.py` defines Great Expectations suites to enforce non‑null constraints, reasonable ranges and uniqueness of keys.  Additional logging examples live in `scripts/logging_sample.py`.
5. **Visualization** – the notebook in `notebooks/` and sample Power BI reports (not included) demonstrate how to consume Gold tables for dashboards.

## Performance Optimization

While the synthetic dataset in this repo is small, the code reflects techniques used to process **tens of terabytes** per month in production:

* **Partitioning & Z‑Ordering** – Delta tables are partitioned by `flight_date` and `airport_code` to prune large datasets and accelerate queries.  Z‑ordering on `flight_id` improves scan efficiency.
* **Auto‑Scaling & Tuning** – Databricks clusters are configured with dynamic autoscaling and tuned Spark configurations (`spark.sql.shuffle.partitions`, `spark.dynamicAllocation.enabled`) to balance cost and latency.
* **Caching & Persistence** – Intermediate DataFrames are cached in memory during transformations to avoid recomputation.  Delta’s **Optimize** and **Vacuum** commands reduce fragmentation and reclaim storage.
* **Efficient Data Formats** – Delta Lake stores data in Parquet with column‑based compression, reducing I/O and enabling ACID transactions.

Applying these techniques in previous roles reduced pipeline latency by **over 60 %** and cut nightly batch runtimes in half.

## Infrastructure as Code

To provision the cloud resources, see the sample Terraform configuration in `infra/main.tf`.  It defines:

* A resource group, storage account and Data Lake container
* An Event Hubs namespace and hub for streaming ingestion
* A Databricks workspace with a dedicated cluster
* An Azure Synapse workspace (placeholder)

This configuration is a starting point; customise the variables and modules to match your subscription.  You can also use Azure Bicep templates if preferred.

## Azure Data Factory Pipeline Sample

In `config/adf_pipeline.json` you’ll find a simplified Data Factory pipeline definition with a single copy activity that ingests events from Event Hubs into Data Lake storage.  In a real deployment you would parameterise dataset paths, configure triggers and add multiple activities (e.g. data flow, notebook execution).  Use this sample as a blueprint when building your own ADF pipelines.

## Logging & Data Quality

Beyond the Great Expectations checks in `scripts/data_quality.py`, the file `scripts/logging_sample.py` illustrates how to write structured logs to Azure Monitor.  By instrumenting your ETL code with custom metrics and exceptions, you can monitor pipeline health, set up alerts and gain visibility into processing times and error rates.

## Getting Started

1. **Install dependencies** – run `pip install -r requirements.txt` in a clean Python environment (conda or venv).  Key packages include PySpark, pandas, matplotlib, great_expectations and azure‑storage‑file‑datalake.
2. **Run the ingestion script** –

   ```bash
   python scripts/ingestion.py --input_flights data/raw/flights.csv \
                              --input_incidents data/raw/incidents.json \
                              --output_path data/staging/bronze
   ```

3. **Transform and aggregate** –

   ```bash
   python scripts/transformation.py --bronze_path data/staging/bronze \
                                   --silver_path data/staging/silver \
                                   --gold_path data/gold
   ```

4. **Data quality** – run `python scripts/data_quality.py --input_path data/staging/silver` to validate your data.
5. **Explore** – open the notebook in `notebooks/` or connect Power BI to the `data/gold` directory to visualise KPIs.

## Sample Data & KPIs

The repository includes synthetic flight and incident datasets with the following characteristics:

* **1 000 flights** across major North American airports and international destinations.
* **Scheduled vs. actual times** allowing computation of departure/arrival delays.
* **Baggage counts** and incident types (e.g. *Baggage Delay*, *Technical Issue*).

From these inputs the pipeline computes metrics such as average departure delay by destination, on‑time performance, incident rate per gate and baggage throughput.  These KPIs mirror those used by airport operations teams to optimise resources and improve the passenger experience.

## Business Impact

Implementing a modern lakehouse architecture has tangible benefits.  In similar projects I have:

* **Reduced reporting latency** from 3 hours to **15 minutes** by migrating to streaming ingestion and Delta Lake.
* **Processed over 30 TB of data per month** with PySpark and autoscaling clusters.
* **Improved SLA compliance** from 92 % to **98 %** by delivering timely incident alerts to gate and baggage teams.
* **Cut nightly batch runtimes by 50 %** by optimising Spark configurations and using partitioned Delta tables.
* **Boosted data quality by 30 %** through automated validation checks and monitoring.

These outcomes highlight the value of a well‑architected data platform for operational excellence.

## Future Enhancements

* **Real‑Time Dashboards** – integrate the Gold tables with a Power BI workspace or Azure Streaming Analytics for live dashboards.
* **Machine Learning** – build predictive models for flight delays or passenger volumes using Databricks and serve them with MLflow.
* **ServiceNow Integration** – replace the synthetic incident dataset with a live ServiceNow connector to reduce latency to minutes.
* **Streaming Ingestion** – connect directly to Event Hubs or Kafka using Spark Structured Streaming for continuous processing.
* **CI/CD Pipelines** – configure GitHub Actions to run unit tests, data quality checks and Terraform deployments on every pull request.

## Resume‑Style Highlights

* **Designed and implemented a real‑time Azure/Databricks data pipeline** ingesting flight and incident data via Event Hubs and ServiceNow, processing over 30 TB of data each month and cutting refresh times from hours to minutes.
* **Developed PySpark ETL workflows** using Delta Lake to transform raw events into curated Silver and Gold tables, applying automated data quality checks to reduce discrepancies by 30 %.
* **Created dashboards and KPIs for operational teams**, leveraging Power BI and Synapse to improve on‑time performance and baggage SLA compliance.
* **Automated cloud resource provisioning with Terraform**, following best practices for infrastructure‑as‑code and CI/CD.
* **Mentored junior engineers** by documenting modular code, providing notebooks and demonstrating reproducible pipelines, improving team productivity.

## Copyright

© 2025 Sushant Koul.  All rights reserved.  Do not redistribute without permission.