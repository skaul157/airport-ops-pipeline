# Realâ€‘Time Airport Operations Data Pipeline

## ðŸ”§ Architecture Overview

The architecture for this project follows a Kappa/Lakehouse pattern with both batch and realâ€‘time ingestion, Delta Lake storage tiers and multiple serving layers.  To make the diagram universally viewable on GitHub without any external images, it is defined using MermaidJS below.  Feel free to copy and edit this diagram in tools like draw.io or diagrams.net if you wish to customise it further.

```mermaid
flowchart LR
    %% Ingestion sources
    subgraph Static_Sources ["Static Data Sources"]
        FTP[FTP / SFTP]
        Database[Relational DB]
        API[REST API]
        FTP --> DF
        Database --> DF
        API --> DF
    end

    subgraph Streaming_Sources ["Stream Data Sources"]
        IoT[IoT / Sensor]
        IoT --> EH
    end

    %% Ingestion services
    DF["Azure Data Factory"]
    EH["Azure Event Hub"]

    %% Storage layers
    Raw["Delta Lake Bronze"]
    Silver["Delta Lake Silver"]
    Gold["Delta Lake Gold"]

    %% Processing
    DB_Batch["Azure Databricks (Batch)"]
    DB_Stream["Azure Databricks (Streaming)"]
    DB_Proc["Azure Databricks (Processing)"]

    %% Serving
    Synapse["Azure Synapse / SQL"]
    BI["PowerÂ BI / Apps"]
    DQ["Monitoring & DQ (Great Expectations / Azure Monitor)"]

    %% Flows
    DF -->|"Batch ingest"| Raw
    EH -->|"Realâ€‘time ingest"| DB_Stream
    Raw -->|"Cleanse"| DB_Batch
    DB_Stream -->|"Parse & enrich"| Silver
    DB_Batch -->|"Parse & enrich"| Silver
    Silver -->|"Aggregate"| DB_Proc -->|"Publish"| Gold
    Gold --> Synapse
    Gold --> BI
    Silver --> DQ
    DB_Stream --> DQ
    DB_Batch --> DQ

    classDef ingest fill:#e7f3fe,stroke:#0078D4;
    classDef process fill:#fdebd0,stroke:#E16D00;
    classDef storage fill:#fff9db,stroke:#C8AB37;
    classDef serving fill:#f5e8f7,stroke:#5C2D91;
    classDef dq fill:#e8f5e9,stroke:#2E7D32;

    class DF,EH,FTP,Database,API,IoT ingest;
    class Raw,Silver,Gold storage;
    class DB_Batch,DB_Stream,DB_Proc process;
    class Synapse,BI serving;
    class DQ dq;
```

## Overview

This project demonstrates how to build a **productionâ€‘ready data pipeline** on Azure using streaming ingestion, robust ETL, Delta Lake storage and interactive dashboards.  The solution draws on my experience modernizing airport analytics platforms: processing **30Â TB+ of operational data per month**, cutting report refresh times from hours to minutes and boosting SLA compliance for baggage and gate teams.  By following the steps in this repository you can stand up a similar pipeline on your own Azure subscription or run it locally with synthetic data.

The goal is to showcase endâ€‘toâ€‘end data engineering capabilities â€“ ingestion, quality checks, transformation, storage optimisation, infrastructure as code and visualization â€“ all within a single, easyâ€‘toâ€‘navigate repo.  The materials here are designed for hiring managers, venture capital partners and technical peers evaluating my expertise in modern data platforms.

## KeyÂ UseÂ Cases

This pipeline addresses several realâ€‘world scenarios encountered at busy airports:

* **Realâ€‘Time Flight Monitoring** â€“ ingest and analyse live departure/arrival events to compute onâ€‘time performance and send alerts when flights are delayed beyond a threshold.
* **Baggage Handling Analytics** â€“ track baggage counts and throughput by flight and terminal to optimise staffing and equipment utilisation.
* **Incident Management & SLA Tracking** â€“ ingest ServiceNow incident tickets (baggage delays, technical issues, passenger disruptions) and correlate them with flight and gate data to monitor SLA compliance.
* **Historical Reporting** â€“ store cleansed and aggregated data in Delta Lake for longâ€‘term analysis and regulatory reporting.

## TechÂ Stack

| Category | Technologies |
|---|---|
| **Ingestion & Orchestration** | AzureÂ EventÂ Hubs, AzureÂ DataÂ Factory, ApacheÂ Kafka (optional) |
| **Processing & Storage** | AzureÂ Databricks (PySpark), DeltaÂ Lake, AzureÂ DataÂ LakeÂ StorageÂ Gen2 |
| **Infrastructure as Code** | Terraform (see `infra/main.tf`) or Bicep |
| **Data Quality & Logging** | GreatÂ Expectations, AzureÂ Monitor (see `scripts/logging_sample.py`) |
| **Visualization & Analytics** | PowerÂ BI, AzureÂ SynapseÂ Analytics |

## Architecture Details

The steps below describe the endâ€‘toâ€‘end flow using Azureâ€‘native components:

1. **Sources** â€“ flight events stream in via **EventÂ Hubs** and operational incidents are captured from **ServiceNow** APIs.
2. **AzureÂ DataÂ Factory** orchestrates ingestion by reading from EventÂ Hubs and landing raw events in the **Bronze** layer of a Data Lake.
3. **AzureÂ Databricks** notebooks (PySpark) parse, clean and join data with reference tables.  The cleansed data forms the **Silver** layer and is validated using **GreatÂ Expectations**.
4. Aggregations such as KPIs and timeâ€‘series metrics are computed in the **Gold** layer and written as Delta tables.  These tables feed **AzureÂ Synapse** for adâ€‘hoc queries and **PowerÂ BI** dashboards for business users.
5. **Monitoring & Logging** using AzureÂ Monitor captures pipeline performance metrics and log events for troubleshooting and alerting.

## PipelineÂ Stages

The code in this repository is organised around the classic Bronzeâ€“Silverâ€“Gold lakehouse pattern:

1. **Ingestion** â€“ `scripts/ingestion.py` reads CSV/JSON files (or streams from EventÂ Hubs) and writes raw Delta tables to `data/staging/bronze`.
2. **Staging & Cleaning** â€“ `scripts/transformation.py` parses timestamps, normalises schemas, handles missing values and joins against reference data such as airport codes.  The resulting Silver tables live in `data/staging/silver`.
3. **Transformation & Aggregation** â€“ the same script aggregates Silver data into Gold tables (e.g. average departure delays, incident rates) stored in `data/gold`.
4. **Data Quality** â€“ `scripts/data_quality.py` defines GreatÂ Expectations suites to enforce nonâ€‘null constraints, reasonable ranges and uniqueness of keys.  Additional logging examples live in `scripts/logging_sample.py`.
5. **Visualization** â€“ the notebook in `notebooks/` and sample PowerÂ BI reports (not included) demonstrate how to consume Gold tables for dashboards.

## PerformanceÂ Optimization

While the synthetic dataset in this repo is small, the code reflects techniques used to process **tens of terabytes** per month in production:

* **Partitioning & Zâ€‘Ordering** â€“ Delta tables are partitioned by `flight_date` and `airport_code` to prune large datasets and accelerate queries.  Zâ€‘ordering on `flight_id` improves scan efficiency.
* **Autoâ€‘Scaling & Tuning** â€“ Databricks clusters are configured with dynamic autoscaling and tuned Spark configurations (`spark.sql.shuffle.partitions`, `spark.dynamicAllocation.enabled`) to balance cost and latency.
* **Caching & Persistence** â€“ Intermediate DataFrames are cached in memory during transformations to avoid recomputation.  Deltaâ€™s **Optimize** and **Vacuum** commands reduce fragmentation and reclaim storage.
* **Efficient Data Formats** â€“ Delta Lake stores data in Parquet with columnâ€‘based compression, reducing I/O and enabling ACID transactions.

Applying these techniques in previous roles reduced pipeline latency by **overÂ 60Â %** and cut nightly batch runtimes in half.

## Infrastructure as Code

To provision the cloud resources, see the sample Terraform configuration in `infra/main.tf`.  It defines:

* A resource group, storage account and Data Lake container
* An EventÂ Hubs namespace and hub for streaming ingestion
* A Databricks workspace with a dedicated cluster
* An AzureÂ Synapse workspace (placeholder)

This configuration is a starting point; customise the variables and modules to match your subscription.  You can also use Azure Bicep templates if preferred.

## AzureÂ DataÂ Factory Pipeline Sample

In `config/adf_pipeline.json` youâ€™ll find a simplified DataÂ Factory pipeline definition with a single copy activity that ingests events from EventÂ Hubs into Data Lake storage.  In a real deployment you would parameterise dataset paths, configure triggers and add multiple activities (e.g. data flow, notebook execution).  Use this sample as a blueprint when building your own ADF pipelines.

## Logging & Data Quality

Beyond the GreatÂ Expectations checks in `scripts/data_quality.py`, the file `scripts/logging_sample.py` illustrates how to write structured logs to AzureÂ Monitor.  By instrumenting your ETL code with custom metrics and exceptions, you can monitor pipeline health, set up alerts and gain visibility into processing times and error rates.

## GettingÂ Started

1. **Install dependencies** â€“ run `pip install -r requirements.txt` in a clean Python environment (conda or venv).  Key packages include PySpark, pandas, matplotlib, great_expectations and azureâ€‘storageâ€‘fileâ€‘datalake.
2. **Run the ingestion script** â€“

   ```bash
   python scripts/ingestion.py --input_flights data/raw/flights.csv \
                              --input_incidents data/raw/incidents.json \
                              --output_path data/staging/bronze
   ```

3. **Transform and aggregate** â€“

   ```bash
   python scripts/transformation.py --bronze_path data/staging/bronze \
                                   --silver_path data/staging/silver \
                                   --gold_path data/gold
   ```

4. **Data quality** â€“ run `python scripts/data_quality.py --input_path data/staging/silver` to validate your data.
5. **Explore** â€“ open the notebook in `notebooks/` or connect PowerÂ BI to the `data/gold` directory to visualise KPIs.

## SampleÂ Data & KPIs

The repository includes synthetic flight and incident datasets with the following characteristics:

* **1Â 000 flights** across major North American airports and international destinations.
* **Scheduled vs. actual times** allowing computation of departure/arrival delays.
* **Baggage counts** and incident types (e.g. *Baggage Delay*, *Technical Issue*).

From these inputs the pipeline computes metrics such as average departure delay by destination, onâ€‘time performance, incident rate per gate and baggage throughput.  These KPIs mirror those used by airport operations teams to optimise resources and improve the passenger experience.

## BusinessÂ Impact

Implementing a modern lakehouse architecture has tangible benefits.  In similar projects I have:

* **Reduced reporting latency** from 3Â hours to **15Â minutes** by migrating to streaming ingestion and Delta Lake.
* **Processed over 30Â TB of data per month** with PySpark and autoscaling clusters.
* **Improved SLA compliance** from 92Â % to **98Â %** by delivering timely incident alerts to gate and baggage teams.
* **Cut nightly batch runtimes by 50Â %** by optimising Spark configurations and using partitioned Delta tables.
* **Boosted data quality by 30Â %** through automated validation checks and monitoring.

These outcomes highlight the value of a wellâ€‘architected data platform for operational excellence.

## FutureÂ Enhancements

* **Realâ€‘Time Dashboards** â€“ integrate the Gold tables with a PowerÂ BI workspace or AzureÂ StreamingÂ Analytics for live dashboards.
* **Machine Learning** â€“ build predictive models for flight delays or passenger volumes using Databricks and serve them with MLflow.
* **ServiceNow Integration** â€“ replace the synthetic incident dataset with a live ServiceNow connector to reduce latency to minutes.
* **Streaming Ingestion** â€“ connect directly to EventÂ Hubs or Kafka using Spark Structured Streaming for continuous processing.
* **CI/CD Pipelines** â€“ configure GitHubÂ Actions to run unit tests, data quality checks and Terraform deployments on every pull request.

## Resumeâ€‘Style Highlights

* **Designed and implemented a realâ€‘time Azure/Databricks data pipeline** ingesting flight and incident data via EventÂ Hubs and ServiceNow, processing over 30Â TB of data each month and cutting refresh times from hours to minutes.
* **Developed PySpark ETL workflows** using Delta Lake to transform raw events into curated Silver and Gold tables, applying automated data quality checks to reduce discrepancies by 30Â %.
* **Created dashboards and KPIs for operational teams**, leveraging PowerÂ BI and Synapse to improve onâ€‘time performance and baggage SLA compliance.
* **Automated cloud resource provisioning with Terraform**, following best practices for infrastructureâ€‘asâ€‘code and CI/CD.
* **Mentored junior engineers** by documenting modular code, providing notebooks and demonstrating reproducible pipelines, improving team productivity.

## Copyright

Â©Â 2025Â SushantÂ Koul.  All rights reserved.  Do not redistribute without permission.